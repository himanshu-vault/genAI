{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d232e362",
   "metadata": {},
   "source": [
    "## Text Generation via OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c39fa",
   "metadata": {},
   "source": [
    "With the OpenAI API, you can use a large language model to generate text from a prompt, as you might using ChatGPT. Models can generate almost any kind of text responseâ€”like code, mathematical equations, structured JSON data, or human-like prose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5c6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf90ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under the shimmering moonlight, the gentle unicorn danced through the enchanted forest, spreading stardust and sweet dreams to every sleepy creature tucked beneath the trees.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90ee6933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"resp_032dfc1e800cd8520069181e3705a881978c4bffcc199fcd85\",\n",
      "  \"created_at\": 1763188279.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"msg_032dfc1e800cd8520069181e37afd081978cbf46609283932d\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"Under the shimmering moonlight, the gentle unicorn danced through the enchanted forest, spreading stardust and sweet dreams to every sleepy creature tucked beneath the trees.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": \"medium\"\n",
      "  },\n",
      "  \"top_logprobs\": 0,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 18,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 32,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 50\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"billing\": {\n",
      "    \"payer\": \"openai\"\n",
      "  },\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"prompt_cache_retention\": null,\n",
      "  \"safety_identifier\": null,\n",
      "  \"store\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background #: bool | NotGiven | None = NOT_GIVEN,\n",
    "# include #: List[ResponseIncludable] | NotGiven | None = NOT_GIVEN,\n",
    "# input #: str | ResponseInputParam | NotGiven = NOT_GIVEN,\n",
    "# instructions #: str | NotGiven | None = NOT_GIVEN,\n",
    "# max_output_tokens #: int | NotGiven | None = NOT_GIVEN,\n",
    "# max_tool_calls #: int | NotGiven | None = NOT_GIVEN,\n",
    "# metadata #: Metadata | NotGiven | None = NOT_GIVEN,\n",
    "# model #: ResponsesModel | NotGiven = NOT_GIVEN,\n",
    "# parallel_tool_calls #: bool | NotGiven | None = NOT_GIVEN,\n",
    "# previous_response_id #: str | NotGiven | None = NOT_GIVEN,\n",
    "# prompt #: ResponsePromptParam | NotGiven | None = NOT_GIVEN,\n",
    "# reasoning #: Reasoning | NotGiven | None = NOT_GIVEN,\n",
    "# service_tier #: NotGiven | Literal['auto', 'default', 'flex', 'scale', 'priority'] | None = NOT_GIVEN,\n",
    "# store #: bool | NotGiven | None = NOT_GIVEN,\n",
    "# stream #: NotGiven | Literal[False] | None = NOT_GIVEN,\n",
    "# temperature #: float | NotGiven | None = NOT_GIVEN,\n",
    "# text #: ResponseTextConfigParam | NotGiven = NOT_GIVEN,\n",
    "# tool_choice #: ToolChoice | NotGiven = NOT_GIVEN,\n",
    "# tools #: Iterable[ToolParam] | NotGiven = NOT_GIVEN,\n",
    "# top_logprobs #: int | NotGiven | None = NOT_GIVEN,\n",
    "# top_p #: float | NotGiven | None = NOT_GIVEN,\n",
    "# truncation #: NotGiven | Literal['auto', 'disabled'] | None = NOT_GIVEN,\n",
    "# user #: str | NotGiven = NOT_GIVEN,\n",
    "# extra_headers #: Headers | None = None,\n",
    "# extra_query #: Query | None = None,\n",
    "# extra_body #: Body | None = None,\n",
    "# timeout #: float | Timeout | NotGiven | None = NOT_GIVEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a04220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_032dfc1e800cd8520069181e3705a881978c4bffcc199fcd85', created_at=1763188279.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_032dfc1e800cd8520069181e37afd081978cbf46609283932d', content=[ResponseOutputText(annotations=[], text='Under the shimmering moonlight, the gentle unicorn danced through the enchanted forest, spreading stardust and sweet dreams to every sleepy creature tucked beneath the trees.', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=18, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=32, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=50), user=None, billing={'payer': 'openai'}, prompt_cache_key=None, prompt_cache_retention=None, safety_identifier=None, store=True)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.retrieve(\"resp_032dfc1e800cd8520069181e3705a881978c4bffcc199fcd85\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4773f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mstreamlit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mst\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      4\u001b[39m client = OpenAI()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "st.title(\"My ChatGPT App\")\n",
    "\n",
    "user_input = st.text_input(\"Ask something\")\n",
    "\n",
    "if st.button(\"Go\"):\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        input=user_input\n",
    "    )\n",
    "    st.write(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e9026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As the moonlight danced on the shimmering lake, a gentle unicorn spread its wings and soared into the night sky, sprinkling stardust over all the dreaming children below.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"Write a one-sentence bedtime story about a unicorn.\"\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cf8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
