{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17a0951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1d37bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.generate(model=\"qwen3:1.7b\", prompt=\"How does moon Glow?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a6a38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moon's \"glow\" can be interpreted in multiple ways, depending on context. Here's a breakdown:\n",
      "\n",
      "### 1. **Natural Glow (Reflection of Sunlight)**  \n",
      "   - The moon's light is **not its own** but the **reflection of sunlight** (a phenomenon called **luminescence**).  \n",
      "   - During the **full moon**, the moon appears brightest because it's facing the Sun directly, maximizing its reflection.  \n",
      "   - During **new moon**, it appears faint or invisible, as it's positioned between the Earth and the Sun.  \n",
      "   - Other phases (waxing crescent, waning gibbous) create varying shades of pale light, often described as a \"glow\" in the night sky.\n",
      "\n",
      "### 2. **Metaphorical or Poetic \"Glow\"**  \n",
      "   - The moon's glow can symbolize **beauty, mystery, or inspiration**. For example:  \n",
      "     - \"The moon's glow whispers secrets to the stars.\"  \n",
      "     - \"Its glow casts a silvery light on the ocean.\"  \n",
      "   - In art or literature, the moon's glow might evoke **nostalgia, tranquility, or a sense of wonder**.\n",
      "\n",
      "### 3. **Scientific Perspective**  \n",
      "   - The moon's glow is **not a light source** but a **reflected light** from the Sun.  \n",
      "   - It appears **pale and diffuse** (like a soft, white light) due to the moon's albedo (reflectivity) and the way light spreads through the atmosphere.\n",
      "\n",
      "### 4. **Cultural or Philosophical Interpretations**  \n",
      "   - In some cultures, the moon's glow is linked to **divine presence**, **nature's cycle**, or **timelessness**.  \n",
      "   - It can also symbolize **hope, renewal, or the passage of time**.\n",
      "\n",
      "### Summary  \n",
      "The moon's \"glow\" is a blend of **natural light reflection**, **phases**, and **symbolic meaning**. Its appearance varies with the moon's position in its orbit, creating a mesmerizing, ever-changing spectacle in the night sky.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c6f71a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The moon's \"glow\" can be interpreted in multiple ways, depending on the context. Here's a breakdown:\n",
      "\n",
      "### 1. **Scientific Explanation**  \n",
      "   - **Reflection of Sunlight**: The moon reflects sunlight, creating a visible glow. This is called **luminescence**.  \n",
      "   - **Color and Appearance**:  \n",
      "     - **White or Pale Glow**: During a **full moon**, the moon appears brightest and whiter, as it reflects direct sunlight.  \n",
      "     - **Reddish Glow**: During a **total lunar eclipse**, the moon appears reddish due to **Earth's atmosphere** scattering sunlight.  \n",
      "     - **Gray or Brown Glow**: In **new moons**, the moon lacks sunlight, but its glow is faint and grayish.  \n",
      "\n",
      "### 2. **Cultural and Symbolic Meanings**  \n",
      "   - **Hope and Beauty**: The moon's glow is often associated with **hope, beauty, or the passage of time** in poetry and folklore.  \n",
      "   - **Nature's Cycle**: Its phases (e.g., waxing, waning) symbolize growth, change, or renewal.  \n",
      "   - **Mythology**: In many cultures, the moon's glow is linked to **deities, spirits, or celestial events** (e.g., the **Goddess of the Moon** in Greek mythology).  \n",
      "\n",
      "### 3. **Astrophysical Perspective**  \n",
      "   - The moon's glow is a **result of sunlight reflection** and the **Earth's atmosphere** (in eclipses).  \n",
      "   - Its light is **not self-emitted** but **reflected** from the sun, making it a **pale, white light**.  \n",
      "\n",
      "### Summary  \n",
      "The moon's glow is a blend of **natural phenomena** (reflection, phases) and **human interpretation** (symbolism, mythology). Whether it's a scientific wonder or a poetic metaphor, the moon's light reminds us of the beauty of celestial cycles and the interplay of light and shadow. ðŸŒ•âœ¨"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model=\"qwen3:1.7b\", prompt=\"How does moon Glow?\", stream=True)\n",
    "for i in response:\n",
    "    print(i['response'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b525fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Happy Birthday to the most amazing person ever! ðŸŽ‚  \n",
      "\n",
      "Wishing you endless joy, endless adventures, and a heart full of laughter. May your day be as bright as the stars and as full of surprises as the best cake ever made. ðŸ°âœ¨  \n",
      "\n",
      "Here's to youâ€”may your every wish come true, your dreams take flight, and your smile light up the world! ðŸŒŸðŸŽˆ  \n",
      "\n",
      "Happy Birthday to you, my favorite person! ðŸŽ‚ðŸ’–  \n",
      "\n",
      "---  \n",
      "*Customize the message with your favorite details or a personal touch!* ðŸ’Œ"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model=\"qwen3:1.7b\", prompt=\"Birthday wish\", stream=True, options={\"temperature\": 0.0,})\n",
    "for i in response:\n",
    "    print(i['response'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6c635e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ Happy Birthday to the best person in the world! ðŸŽ‚  \n",
      "\n",
      "May this special day be filled with laughter, joy, and moments that make your heart shine. Wishing you endless happiness, good health, and all the success youâ€™ve ever dreamed of. ðŸŒŸ  \n",
      "\n",
      "Your smile lights up the room, and your spirit never fades. Hereâ€™s to a year of adventures, love, and unforgettable memories! ðŸŒˆ  \n",
      "\n",
      "Happy Birthday to you, my favorite superhero! ðŸš€ðŸŽˆ  \n",
      "\n",
      "Warmest wishes,  \n",
      "[Your Name]  \n",
      "\n",
      "---  \n",
      "*Feel free to customize it with personal details or a favorite quote! ðŸ’Œ*"
     ]
    }
   ],
   "source": [
    "response = ollama.generate(model=\"qwen3:1.7b\", prompt=\"Birthday wish\", stream=True, options={\"temperature\": 1,})\n",
    "for i in response:\n",
    "    print(i['response'], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7725f10e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (31.2 GiB) than is available (10.6 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     image_bytes= f.read()    \u001b[38;5;66;03m# READING THE IMAGE \u001b[39;00m\n\u001b[32m      8\u001b[39m image_64= base64.b64encode(image_bytes).decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# ENCODING THE IMAGE IN BASE 64 BITS.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m response= \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mqwen3-vl:2b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage_64\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGive caption for the image.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\local_synced\\workspace\\genAI\\venv\\Lib\\site-packages\\ollama\\_client.py:262\u001b[39m, in \u001b[36mClient.generate\u001b[39m\u001b[34m(self, model, prompt, suffix, system, template, context, stream, think, logprobs, top_logprobs, raw, format, images, options, keep_alive)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n\u001b[32m    234\u001b[39m   \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    235\u001b[39m   model: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    250\u001b[39m   keep_alive: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    251\u001b[39m ) -> Union[GenerateResponse, Iterator[GenerateResponse]]:\n\u001b[32m    252\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[33;03m  Create a response using the requested model.\u001b[39;00m\n\u001b[32m    254\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    259\u001b[39m \u001b[33;03m  Returns `GenerateResponse` if `stream` is `False`, otherwise returns a `GenerateResponse` generator.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGenerateResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGenerateRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m      \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m      \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m      \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m      \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m      \u001b[49m\u001b[43mthink\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthink\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m      \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m      \u001b[49m\u001b[43mraw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m      \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_copy_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_alive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_dump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_none\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\local_synced\\workspace\\genAI\\venv\\Lib\\site-packages\\ollama\\_client.py:189\u001b[39m, in \u001b[36mClient._request\u001b[39m\u001b[34m(self, cls, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**part)\n\u001b[32m    187\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.json())\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\local_synced\\workspace\\genAI\\venv\\Lib\\site-packages\\ollama\\_client.py:133\u001b[39m, in \u001b[36mClient._request_raw\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    131\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.ConnectError:\n\u001b[32m    135\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(CONNECTION_ERROR_MESSAGE) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mResponseError\u001b[39m: model requires more system memory (31.2 GiB) than is available (10.6 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import ollama\n",
    "\n",
    "image_path= \"Linkedin.jpg\"\n",
    "\n",
    "with open(image_path,\"rb\") as f: # OPENING THE IMAGE IN READ BINARY \n",
    "    image_bytes= f.read()    # READING THE IMAGE \n",
    "image_64= base64.b64encode(image_bytes).decode(\"utf-8\") # ENCODING THE IMAGE IN BASE 64 BITS.\n",
    "\n",
    "response= ollama.generate(model=\"qwen3-vl:2b\", images=[image_64], prompt=\"Give caption for the image.\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c115acf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caad8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a09ea423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking:\n",
      " Okay, so I need to figure out how many times the letter 'r' appears in the word \"strawberry\". Let me start by writing down the word and then going through each letter one by one.\n",
      "\n",
      "First, let's write out the word: S-T-R-A-W-B-E-R-R-Y.\n",
      "\n",
      "Now, I'll break it down letter by letter. Let me count each letter:\n",
      "\n",
      "1. S\n",
      "2. T\n",
      "3. R\n",
      "4. A\n",
      "5. W\n",
      "6. B\n",
      "7. E\n",
      "8. R\n",
      "9. R\n",
      "10. Y\n",
      "\n",
      "So, the letters are S, T, R, A, W, B, E, R, R, Y. Now, I need to check which of these are 'r's. Let's go through them again:\n",
      "\n",
      "- First letter: S â€“ not an 'r'.\n",
      "- Second letter: T â€“ not an 'r'.\n",
      "- Third letter: R â€“ that's an 'r'.\n",
      "- Fourth letter: A â€“ not an 'r'.\n",
      "- Fifth letter: W â€“ not an 'r'.\n",
      "- Sixth letter: B â€“ not an 'r'.\n",
      "- Seventh letter: E â€“ not an 'r'.\n",
      "- Eighth letter: R â€“ that's another 'r'.\n",
      "- Ninth letter: R â€“ another 'r'.\n",
      "- Tenth letter: Y â€“ not an 'r'.\n",
      "\n",
      "So, counting the 'r's: the third, eighth, and ninth letters are all 'r's. That's three 'r's in total. Wait, let me double-check to make sure I didn't miss any. Let me write the word again with positions:\n",
      "\n",
      "1: S\n",
      "2: T\n",
      "3: R\n",
      "4: A\n",
      "5: W\n",
      "6: B\n",
      "7: E\n",
      "8: R\n",
      "9: R\n",
      "10: Y\n",
      "\n",
      "Yes, positions 3, 8, and 9 are 'r's. So that's three 'r's. Hmm, but sometimes people might miscount, especially if they rush through. Let me confirm by looking at the word again: \"strawberry\". Let me spell it out: S-T-R-A-W-B-E-R-R-Y. Yes, that's correct. So the letters are S, T, R, A, W, B, E, R, R, Y. So three 'r's. Wait, but sometimes when people say \"strawberry\", they might think it's \"strawberry\" with an 'e' instead of an 'r', but in this case, the word is \"strawberry\" with an 'e' at the end. So the spelling is correct. So the answer should be three 'r's.\n",
      "\n",
      "But wait, let me check once more. Let me count again:\n",
      "\n",
      "S-T-R-A-W-B-E-R-R-Y.\n",
      "\n",
      "Breaking it down:\n",
      "\n",
      "- S, T, R (1st r)\n",
      "- A, W, B, E, R (2nd r)\n",
      "- R (3rd r)\n",
      "\n",
      "So yes, three 'r's. I think that's right. I don't see any other 'r's. So the answer is 3.\n",
      "\n",
      "Answer:\n",
      " To determine how many times the letter **'r'** appears in the word **\"strawberry\"**, we begin by writing out the word and analyzing each letter individually:\n",
      "\n",
      "**strawberry**  \n",
      "Letters: S, T, R, A, W, B, E, R, R, Y\n",
      "\n",
      "Now, we identify the positions of the letter **'r'**:\n",
      "\n",
      "1. Third letter: **R**\n",
      "2. Eighth letter: **R**\n",
      "3. Ninth letter: **R**\n",
      "\n",
      "---\n",
      "\n",
      "### Final Count:\n",
      "There are **three** occurrences of the letter **'r'** in the word **\"strawberry\"**.\n",
      "\n",
      "---\n",
      "\n",
      "### âœ… Final Answer:\n",
      "$$\n",
      "\\boxed{3}\n",
      "$$\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "  model='qwen3:1.7b',\n",
    "  messages=[{'role': 'user', 'content': 'How many letter r are in strawberry?'}],\n",
    "  think=True,\n",
    "  stream=False,\n",
    ")\n",
    "\n",
    "print('Thinking:\\n', response.message.thinking)\n",
    "print('Answer:\\n', response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc8bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thinking:\n",
      "Okay, so I need to figure out what 17 multiplied by 23 is. Let me think about how to approach this. I remember that multiplication can be done in different ways, like using the standard algorithm or maybe breaking down the numbers into smaller parts. Let me try both methods to check if they give the same answer.\n",
      "\n",
      "First, maybe I can use the standard multiplication method. Let me write it down. So, 17 times 2"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "\n",
    "stream = chat(\n",
    "  model='qwen3:1.7b',\n",
    "  messages=[{'role': 'user', 'content': 'What is 17 Ã— 23?'}],\n",
    "  stream=True,\n",
    ")\n",
    "\n",
    "in_thinking = False\n",
    "content = ''\n",
    "thinking = ''\n",
    "for chunk in stream:\n",
    "  if chunk.message.thinking:\n",
    "    if not in_thinking:\n",
    "      in_thinking = True\n",
    "      print('Thinking:\\n', end='', flush=True)\n",
    "    print(chunk.message.thinking, end='', flush=True)\n",
    "    # accumulate the partial thinking \n",
    "    thinking += chunk.message.thinking\n",
    "  elif chunk.message.content:\n",
    "    if in_thinking:\n",
    "      in_thinking = False\n",
    "      print('\\n\\nAnswer:\\n', end='', flush=True)\n",
    "    print(chunk.message.content, end='', flush=True)\n",
    "    # accumulate the partial content\n",
    "    content += chunk.message.content\n",
    "\n",
    "  # append the accumulated fields to the messages for the next request\n",
    "  new_messages = [{ 'role': 'assistant', thinking: thinking, content: content }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c587eef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
